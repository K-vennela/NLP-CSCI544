Hi Mike,
we've recently been making plans with Simon Tett at the Hadley Centre for comparing model simulations with various climate reconstructions, including the MBH98 and MBH99 Northern Hemisphere temperatures.  I was stressing the importance of including uncertainty estimates in the comparison and that the error estimates should depend on the timescale (e.g. smoothing filter or running mean) that had been applied.
I then looked at the file that I have been using for the uncertainties associated with MBH99 (see attachment), which I must have got from you some time ago.  Column 1 is year, 2 is the "raw" standard error, 3 is 2*SE.
But what are columns 4 and 5?  I've been plotting column 4, labelled "1 sig (lowf)" when plotted your smoothed reconstruction, assuming that this is the error appropriate to low-pass filtered data.  I'd also assumed that the last column "1 sig (highf)" was appropriate to high-pass filtered data.  I also noticed that the sum of the squared high and low errors equalled the square of the raw error, which is nice.
But I've realised that I don't understand how you estimate these errors, nor what time scale the lowf and highf cutoff uses (maybe 40-year smoothed as in the IPCC plots?).  From MBH99 it sounds like post-1600 you assume uncorrelated gaussian calibration residuals.  In which case you would expect the errors for a 40-year mean to be reduced by sqrt(40).  This doesn't seem to match the values in the attached file.  Pre-1600 you take into account that the residuals are autocorrelated (red noise rather than white), so presumably the reduction is less than sqrt(40), but some factor (how do you compute this?).
The reason for my questions is that I would like to (1) check whether I've been doing the right thing in using column 4 of the attached file with your smoothed reconstruction, and (2) I'd like to estimate the errors for a range of time scales, so I can compare decadal means, 30-year means, 50-year means etc.
Thanks in advance for any help you can give me here.
Tim